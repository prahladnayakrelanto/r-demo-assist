{
  "slides": [
    {
      "slideNumber": 1,
      "content": "Case Studies – Data & Analytics Copyright © 2024 Relanto Inc. All rights reserved."
    },
    {
      "slideNumber": 2,
      "content": "BI Transformation: Data Quality, Governance, Self-Service BI Data Quality Fragmented sources, limited data governance, and less visibility to data lineage. Delays in reporting KPIs Time-consuming Business – IT collaboration for interactive dashboard creation due to dependency on IT and lack of reusability Significant discrepancy between business-driven KPIs and the insights obtained from reporting leading to a substantial gap between anticipated business expectations and the actual BI dashboard output. Lack of prioritization in adopting new technologies for cloud data services and BI tools, coupled with the absence of an agile framework and process for efficient solution delivery Deployed a cloud-based data pipeline with AWS S3, Data Glue, Data Brew, and Redshift Real-time reporting with business KPIs with enhanced self-service BI capabilities using Tableau . Streamlined data governance and provided a comprehensive view of data assets and lineage Data Quality improvement using AWS Data Glue and Data Brew Implementation of SSOT in Redshift combining various data sources improving trust in data Self-Service BI using advanced features of Tableau (NLQ)- Report Generation by Business users without IT involvement Data Catalogue for tracking the lineage and data traceability Established Agile BI methodology Project Overview : A leading hi-tech client faced challenges within their Business Intelligence (BI) system. Business leaders were concerned about the quality, accuracy, and timeliness of Key Performance Indicators (KPIs)  and turnaround times for new reports. The client was seeking a strategic partner to address these issues by implementing a robust data pipeline with a single source of truth (SSOT), enhancing data quality, transparency in data governance, tracking data lineage, and establishing a self-service BI solution to reduce IT dependency for business users. Improved data quality and Governance Real-time Reporting on Business  KPI Enhanced self-service BI capabilities through advanced tool Enabled transparency and compliance with detailed data lineage tracking."
    },
    {
      "slideNumber": 3,
      "content": "E nhancing Data Management Efficiency with Automation & AI Data Quality Issues: Inconsistent, duplicate, and missing data led to inaccurate risk models and unreliable insights. Traditional data cleansing methods were reactive, slowing down analytics and decision-making. Siloed & Inefficient Data Access: Business users faced delays in accessing governed datasets, slowing AI and analytics adoption. Role-based access controls were manually configured, increasing security risks. Operational Inefficiency: Data teams spent excessive time building ad-hoc pipelines instead of focusing on innovation. Compliance reporting required extensive manual efforts, increasing operational costs. Deployed AI-driven anomaly detection using Great Expectations & Talend to auto-flag missing or incorrect data. Used ML-powered data profiling to enforce data validation rules and detect duplicate entries. Developed template-based pipelines in Terraform & Snowflake to accelerate onboarding of new datasets. Enabled Data Mesh architecture to allow teams to self-serve governed datasets securely. Deployed AI-driven audit logs to track data lineage, ensuring full transparency for regulators. Project Overview : A leading hi-tech organization wanted to s treamline data management, improve data quality, and enable AI-driven decision-making through automation 70% improvement in data quality issues 40% increase in compliance efficiency Stronger security and access control 30% Reduction in Operational costs"
    },
    {
      "slideNumber": 4,
      "content": "Self service Data Catalogue and Data Quality assurance Enterprise Data Warehouse Application data store (Oracle) Source: Bookings Revenue Product Data Feed: 18 Feeds / day Records: 300M/day Transformation & Summarize: Revenue Backlog Quota Sheet Compensation Data Ingestion Size: 4 GB / day Records: 300M / day Refresh: Daily Build ES Indexes: Backlog Summary Backlog Details Order/Adj Search 5 indexes APIs 36 APIs 31 Non ES 5 ES UI 10 pages Sub second response Data Processing Processing Time : 4Hrs/day Processing: Daily On Prem Cloud Denorm for ES: Backlog Summary Backlog Details Order Search Order Adjustment Order Denorm : Size : 45 GB Records: 2 billion Backlog Denorm : Size: 4 GB Records: 160 million Hadoop Tech stack ( Spark , Hive, Sqoop) , Oracle , Elastic search , MuleSoft, Angular , TD, Informatica Number of users : 17000 System Architecture System design/development/QA Deployment / Maintenance / Support Team : Onsite : 3 , Offshore : 15 (steady state) Duration : 12 month Model : DevOps"
    },
    {
      "slideNumber": 5,
      "content": "A leading SaaS company with a global user base of millions faced significant challenges in monitoring application performance, tracking user interactions, and rapidly diagnosing issues. Ensuring a seamless user experience was critical, but the lack of real-time visibility into system health and user behavior made it difficult to proactively address performance bottlenecks and operational inefficiencies. Limited Observability: The existing monitoring framework lacked end-to-end visibility into application performance and user interactions, hindering proactive anomaly detection and incident response. Fragmented Data Sources: Metrics, logs, and traces were scattered across multiple tools, complicating data correlation and prolonging the troubleshooting process. Scalability Concerns: As the platform scaled, the observability stack struggled to keep pace, leading to potential blind spots in performance monitoring and system reliability. Datadog for Unified Monitoring: Implemented Datadog to leverage real-time monitoring, APM, and distributed tracing, enabling deep visibility into application performance. Its extensive integrations ensured seamless telemetry ingestion across the technology stack. SignalFx for Advanced Visualization and Alerting: Deployed SignalFx to build dynamic dashboards and establish real-time, anomaly-driven alerting. Its ability to process high-resolution metrics at scale enhanced performance monitoring and expedited issue detection. Splunk for Centralized Logging: Integrated Splunk for log aggregation and advanced indexing, streamlining log correlation and root cause analysis. This ensured robust compliance with data retention policies while enhancing investigative efficiency. Observability: Datadog | SignalFx |Splunk 45% decrease in critical incidents through proactive alerting 70% reduction in Mean Time To Resolution (MTTR) 99.95% uptime achieved (exceeding initial SLAs) Enabled data-driven capacity planning and optimization Consolidated view for both technical and business stakeholders"
    },
    {
      "slideNumber": 6,
      "content": "Data Mesh: Scaling Data Mgmt. & Customer Support A prominent provider of collaboration and productivity software has expanded its product portfolio over the past two decades through both internal development and strategic acquisitions. However, this expansion introduced a diverse array of products, each with unique data structures and operational models. The company initially relied on a monolithic billing system, which struggled to keep pace with the increasing complexity and diversity of its expanding product ecosystem. Enhanced Data Consistency: The data mesh architecture standardized formats, ensuring accurate and reliable data representation. Improved Data Accessibility: Decentralized data ownership empowered domain teams, streamlining data discovery and integration. Reduced Latency: Real-time processing enabled faster insights, ensuring customer support teams had up-to-date information. Scalability: The modular microservices architecture supported seamless expansion while maintaining high system performance. Operational Efficiency: Offloading processing tasks to microservices optimized system responsiveness and performance. Enhanced Customer Satisfaction: Timely, accurate data availability enabled faster issue resolution, improving customer experience. Scattered Data Sources: Dispersed data made aggregation and unified access difficult. Non-Source Agnostic Systems: Frequent source changes required constant back-end updates, increasing maintenance. Heavy Back-End Services: Intensive data processing caused latency and reduced responsiveness. Latency and Stale Information: Batch processing delayed data availability, leading to outdated insights. High Resolution Time: Inefficient data handling prolonged customer issue resolution. To address these challenges, the client adopted a data mesh with domain-driven decentralization and a product-focused approach. Streaming: AWS Kinesis enabling real-time data ingestion, ensuring immediate processing and analysis. Queue Management: AWS SQS handling message queues efficiently, ensuring reliable service communication. Microservices: Flask-based microservices facilitating modular and scalable data processing workflows. Storage Solutions: AWS S3: Storing raw and processed data securely with scalable infrastructure. DynamoDB: Managing semi-structured data with flexible and efficient querying. Elasticsearch: Indexing and retrieving processed data rapidly with customized search capabilities. Analytics Platform: Databricks supporting advanced data processing, analytics, and collaborative data science."
    },
    {
      "slideNumber": 7,
      "content": "Case Study: B2B Sales Agency – Data-Driven Incentives Redevelop their entire data pipeline from scratch. Developed Python-based data extractors + Fivetran , data transformation scripts, and a rich set of data models. The models are being used by their BI tool to provide accurate, timely reporting for all of their users and clients. Some of the salient aspects of our implementation are: Lack of reliable reporting metrics for their clients. Inefficient lead management due to high volume and complexity. Lack of reliable performance tracking across multiple subdivisions and teams. Difficulties in accurate incentive calculation leading to sales development representative (SDR) dissatisfaction and higher churn rates. Improved Data Management: Data quality issues were reduced by 90%, leading to a more reliable lead management system capable of handling high volumes of data. Enhanced Performance Tracking : Introduced a unified performance tracking system that boosted reporting accuracy across subdivisions close to 100%, enhancing strategic decision-making. Incentive Accuracy: Refined the incentive calculation process, resulting in a 60% decrease in discrepancies and a notable increase in SDR satisfaction and retention. Operational Efficiency: Streamlined data extraction and transformation processes reduced their revenue & sales ops calculations by 25% every month. Client Reporting: Enhanced the BI tool's reporting capabilities, leading to an increase in client satisfaction due to timely and accurate reporting. Our Solution Results The Problem Business - specific data models on dbt Automatic data quality checks with proactive alerting for data quality issues Retention and backup policies Timely error logging & and alerting on every part of the data pipeline The ability to implement data corrections from user errors seamlessly and easily Customer Information: The client is a large B2B sales agency with over 50 clients to cater to. Major challenges were with data quality, sales performance tracking, and reporting for their clients."
    },
    {
      "slideNumber": 8,
      "content": "Data and Cloud Migration Business Context: The client was utilizing a data-intensive application with complex workflows and operations. They were facing several challenges due to the increasing demand for transactions, which required scalable and high-performance solutions. To support future use cases, improve performance, and increase scalability, the client was looking for a reliable partner to help them migrate the system to the cloud. RELANTO’S INVOLVEMENT Database System Implement a robust and reliable database using AWS RDS system to transfer all the data from the on-premises databases to the new Postgres databases on AWS which improves data management and ensures that the application's data is re-stored securely This migration ensures data consistency and accuracy Host the UI on AWS S3 which is designed to deliver high-performance web applications UI Hosting Migration to Cloud Hub All the on-prem MuleSoft APIs are migrated to Cloud Hub leveraging the latest MuleSoft version to scale the application and support more transactions Integrate with the Salesforce Cloud Platform and Heroku to ensure seamless data flow between systems Automate CI/CD Automate CI/CD pipeline using Circleci , GitHub, TeamCity, and Octopus to ensure a smooth and efficient deployment process This automation reduces manual errors, improves efficiency, and increases deployment speed Reduced cost of operations with the adoption of the Global Delivery Model Increased user satisfaction with timely resolution of issues Migrated 30+ Mule Apps, 100+ complex workflows to AWS Improved data consistency and accuracy"
    },
    {
      "slideNumber": 9,
      "content": "Data visualizations for AI/ML Insights adoption Project Overview : Customer Segmentation and Opportunity assessment For An Information Technology And Networking Giant Numerous factors considered to identify customers who share similarities in terms of their likelihood of buying similar product - Industry /Market , Size /Scale, Market Demographics, Distinctive Attributes The output of the segmentation is presented on a Tableau Dashboard, which visually highlights the key differences among segments and aligns them with business objectives. Using segmentation and AI/ML techniques, algorithms analyze variations in customer’s Bookings patterns with reference to similar customer from segment and predicted cross-selling upselling opportunities Identify customer with similar businesses as they share common characteristics and face similar challenges and often tend to have similar product needs. Python Snowflake Tableau Enhanced Customer Experience : Personalizing product recommendations by understanding the specific needs of different segments. Automation of insight for new customers and changes in customer data Product Sales: With improved targeting, the company can drive higher conversion rates and increase overall sales. Interpretability: ML models, operate as \"black boxes,\" making it difficult to understand how they make decisions. This lack of transparency can hinder trust and adoption."
    },
    {
      "slideNumber": 10,
      "content": "Seller Compensation Reporting Tool Project Overview : Enhance tool to use latest tech stack to improve data consistency across the screens, minimizing end to end data refresh cycles  and improving data auditing and monitoring capabilities Integration and Summarization Improvements:​ Enabled Snowflake (SF) ingestion for critical data objects, allowing data summarization at varying granularity tailored to screen flow needs.​ Achieved seamless integration of ingestion and daily summarization processes, ensuring consistency across different UI screens.​ API integration and data separation: Reduced transaction volume between the metric platform, compensation systems, and reporting applications by separating summarized transactions at different levels (order/line/SKU) and linking them to detailed transactions via REST API.​ Cost Control and Efficiency:​ Managed high data volumes effectively—regular flow involves 300-500k transactions, with spikes above 1 million during month-end/quarter-end or re-processing.​​ Data Consistency and Refresh Cycles Inconsistent data across user interfaces Long data refresh cycles delaying timely information availability. High Transaction Volume Management: Ensuring performance and reliability during high transaction periods Complex Data Integration Integrating data from multiple sources seamlessly. Maintaining high performance and low latency. Cost Control and Efficiency Balancing operational costs with system efficiency. Optimizing resources during peak transaction periods without compromising data integrity. Snowflake Tableau Enhanced Customer Experience : Personalizing product recommendations by understanding the specific needs of different segments. Automation of insight for new customers and changes in customer data Product Sales: With improved targeting, the company can drive higher conversion rates and increase overall sales."
    },
    {
      "slideNumber": 11,
      "content": "Unified and friendly Partner experience Increased business via Partner network Reduced delays and inaccuracies in payments and accruals Reduced operational cost and increased scalability Data Engineering Solution Project Overview: A leading technology company faced several challenges in automating the rebate and payment process for VIP Annuity partners and automating the accruals for finance forecasting. The client was looking for a partner with expertise in data engineering solutions to automate partner rebates, payments, and accruals for finance forecasting. Develop a robust data integration process to consolidate data from multiple sources into a single data repository/data warehouse using AWS glue, Redshift which is designed to accommodate the specific requirements of the rebate computation process. Establish a data governance framework to ensure that the data used for rebate computation is accurate, consistent, and trustworthy. Integrate various sources of data for the enriched data to be stored in Amazon Redshift as the single source of truth for all partner and rebate-related data using AWS Athena to analyze the data enabling consumption by various other systems . Utilized Tableau to provide insights into the data  supporting business decision-making. Implement a data quality monitoring process using AWS Cloud watch to ensure that the data is of high quality and meets the necessary standards for accuracy and completeness. The process includes the identification and correction of data anomalies, inconsistencies, and errors. RELANTO’S INVOLVEMENT"
    },
    {
      "slideNumber": 12,
      "content": "Next-Generation Sales Forecasting Larger volume of data impacting the performance of data load Complex business logics for fair market value Requirement for critical application with accurate data for forecasting Redundant bookings and opportunity pipelines system Understanding the data architecture of upstream systems to pull relevant data Creation of parallel pipelines to speed up the ingestion in Informatica Enable continuous monitoring and alerts on jobs pulling latest bookings and pipeline numbers to ensure the forecast is accurate Ensure always-on support so that tickets are resolved within the given SLA Client Overview : A leading hi-tech client was looking for a solution that provides sellers with access to their past week’s booking information for various territorial hierarchies encompassing both products and services. To meet their requirements, the client requested the development of three separate pipelines to handle seven quarters of historical booking data, current quarter bookings data, and plan data sourced from their Enterprise Data Warehouse. These pipelines need to refresh on a daily basis and seamlessly integrate with a next-generation forecasting application. Increased user satisfaction with accurate Bookings data for Forecasting. Global history data for past metrics and future Sales Plan Analytics on past data to project trends in Sales Faster data load to help the Sales team with timely Forecasting"
    },
    {
      "slideNumber": 13,
      "content": "Data and Cloud Migration Business Context: The client was utilizing a data-intensive application with complex workflows and operations. They were facing several challenges due to the increasing demand for transactions, which required scalable and high-performance solutions. To support future use cases, improve performance, and increase scalability, the client was looking for a reliable partner to help them migrate the system to the cloud. RELANTO’S INVOLVEMENT Database System Implement a robust and reliable database using AWS RDS system to transfer all the data from the on-premises databases to the new Postgres databases on AWS which improves data management and ensures that the application's data is re-stored securely This migration ensures data consistency and accuracy Host the UI on AWS S3 which is designed to deliver high-performance web applications UI Hosting Migration to Cloud Hub All the on-prem MuleSoft APIs are migrated to Cloud Hub leveraging the latest MuleSoft version to scale the application and support more transactions Integrate with the Salesforce Cloud Platform and Heroku to ensure seamless data flow between systems Automate CI/CD Automate CI/CD pipeline using Circleci , GitHub, TeamCity, and Octopus to ensure a smooth and efficient deployment process This automation reduces manual errors, improves efficiency, and increases deployment speed Reduced cost of operations with the adoption of the Global Delivery Model Increased user satisfaction with timely resolution of issues Migrated 30+ Mule Apps, 100+ complex workflows to AWS Improved data consistency and accuracy"
    },
    {
      "slideNumber": 14,
      "content": "Cloud Migration for Hi-Tech Industry Integration BENEFITS DELIVERED Migration Business Context: A leading technology company specializing in the development, manufacturing, & networking of hardware, telecommunications equipment, & other high-tech products for various industries. The client wanted to adopt a standardized approach to migrate from an on-premises database to the cloud and looking for a partner who could help them in identifying the right solution and methodologies to migrate to the cloud. Migrate Data from on-premise Oracle and Casagrande database to Postgres in the cloud using AWS S3 for archives and Snowflake for downstream data movement Migrate AWS to GCP Cloud Integrate Salesforce data into a heterogeneous database using Informatica workflows Establish ETL pipelines and triggers to monitor and ensure smooth performance TOOLS UTILIZED Decrease in data processing time. Reduction in server load and storage costs Reduced data transfer errors through automation and ETL pipelines 12000 users handled Data velocity >  2.5 M transactions/week Performance Data Load Perform data load process that involves both full and incremental data loads, which are managed through scheduled jobs To streamline the data transfer process and ensure efficient migration to the cloud, a load automation process is established using Scala & Spark Fine-tune the queries and optimize the loading performance Migrate to more efficient database structures and automate performance measurements Conduct both internal and external performance audits to ensure optimal performance RELANTO’S INVOLVEMENT"
    },
    {
      "slideNumber": 15,
      "content": "Cisco -CPF Heavy reliance on partner network as generating 85%+ of their bookings through strong Partner network for bookings and managing co-invested Channel Development Funds for practice building and sales acceleration. Challenges in managing 11 programs with diverse objectives targeted towards different types of business and sell-through models, while lacking a single source of truth for funds planning, allocation, and distribution of over $328M in development funds. To address these challenges,  We proposed a scope of work (SOW) including allocation management and distribution that encompasses the planned activities for integrating the shortlisted WorkSpan SSOT System into the client enterprise IT infrastructure. To drive sales growth, CPF implemented partner incentives with the primary objectives of reducing funds leakage, improving ROI on funds utilization, and achieving productivity gains in field, finance, and operations. Client Overview : A hi-tech client has a program for Partner Funds aimed at driving sales growth, which involves consolidating all CPF sub-programs onto a single platform and digitizing manual processes. The client faced the challenge of managing 11 programs with diverse objectives targeted towards different types of business and sell-through models. Consolidated 11 funds worth ~$389M, reducing funds leakage by ~$13.2M. Improved ROI on funds utilization by >3%. Streamlined funding operations, reducing cycle time by 3-7 days on average. Improved productivity ( field, partner, finance, and operations.) Scalable solution serving over 15K users and partners across 6 theatres."
    },
    {
      "slideNumber": 16,
      "content": "Cisco -PIVOT Business Context: Digital transformation of Cisco’s Internal channel analytics platform (Tableau based live reporting) and grow stakeholder adoption globally. Implemented project management rigor and adopted Agile Scrum methodology to drive delivery or features and new datasets while providing transparency to Stakeholders, IT and Management. 100% modeling and design achieved in the Partner system. Incorporation of numerous metrics for comprehensive analysis. Regular release of approximately 10-15 tasks per month and addressed 5 to 10 problem statements for end users. Continuous delivery of new features and improvements. Implemented database system capable of migrating data from various heterogeneous source systems. This migration was accomplished using the BusinessObjects Data Services (BODS) platform, which facilitated data integration, data quality management, data profiling, and data processing. Leveraged BODS to integrate and transform trusted data into a data warehouse system for analytical reporting within SAP HANA. To enable more advanced data slicing and manipulation, implemented calculation views in the SAP HANA database. These calculation views serve as flexible information views that provide the ability to define sophisticated data subsets and aggregations and created column views are stored in the \"SYS_BIC\" schema, ensuring easy access and management of the data views."
    }
  ]
}